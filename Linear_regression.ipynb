{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "31cd3fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "da9fe2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[1,1],[2,2],[3,3],[4,4],[5,5]])\n",
    "y=np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1824c187",
   "metadata": {},
   "source": [
    "## Two Methords of implementing Linear Regression\n",
    "\n",
    "1. Iterative methord\n",
    "2. Closed form solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e63083",
   "metadata": {},
   "source": [
    "### Closed Form Solution:-\n",
    "#### Linear Algebra approach\n",
    "Ax=B\n",
    "linear combination of A should be as close as possible to B, if B in space of A then solution is possible simply by solving x for Ax=B, else need to find the closest space of B that is in column space of A\n",
    "and that closest space is when B is projected onto column space of A  <br>\n",
    "Ax=PB (projecting B into A's subspace)<br>\n",
    "Ax=A(ATA)-1ATB <br>\n",
    "so X=(ATA)-1ATB <br>\n",
    "\n",
    "#### Calculus approach:\n",
    "find minima by using dl/dw=0 where L= mean squared loss= (y-wx)T(y-wx)\n",
    "\n",
    "#### probability/statistics approach (MLE)\n",
    "y=wx+E where E is assumed to be a gaussian noise So inorder to have minimum noise we need to best approximate the parameter w\n",
    "so maximum likely hood estimate of normal curve will finally lead to minimization of (y-wx)^2 (mean squared loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f6e131b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 1]\n",
      " [3 3 1]\n",
      " [4 4 1]\n",
      " [5 5 1]]\n"
     ]
    }
   ],
   "source": [
    "# add 1 to get intercept term\n",
    "X=np.append(X,axis=1,values=[[1],[1],[1],[1],[1]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d8850660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5 0.5 0. ]\n"
     ]
    }
   ],
   "source": [
    "W=np.linalg.pinv(X.T@X)@(X.T@y)  #pinv is pseudo inverse\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "79c1c387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.999999999999997\n"
     ]
    }
   ],
   "source": [
    "pred=W@np.array([6,6,1])\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21627295",
   "metadata": {},
   "source": [
    "## iterative solution\n",
    "dw=(dL/dw)   L=loss function (y-ypred)^2/n (mean squared error) <br>\n",
    "dl/dw=(2/n)*(y-ypred).X \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11e0f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 11.0000\n",
      "Epoch 100: Loss = 0.0016\n",
      "Epoch 200: Loss = 0.0008\n",
      "Epoch 300: Loss = 0.0004\n",
      "Epoch 400: Loss = 0.0002\n",
      "Epoch 500: Loss = 0.0001\n",
      "Epoch 600: Loss = 0.0000\n",
      "Epoch 700: Loss = 0.0000\n",
      "Epoch 800: Loss = 0.0000\n",
      "Epoch 900: Loss = 0.0000\n",
      "\n",
      "Final Parameters (theta): [[0.49945628]\n",
      " [0.49945628]\n",
      " [0.0039555 ]]\n",
      "Predictions: [[1.00286806]\n",
      " [2.00178063]\n",
      " [3.00069319]\n",
      " [3.99960576]\n",
      " [4.99851833]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs\n",
    "X = np.array([[1, 1],\n",
    "              [2, 2],\n",
    "              [3, 3],\n",
    "              [4, 4],\n",
    "              [5, 5]])\n",
    "y = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # shape (5, 1)\n",
    "\n",
    "# Add intercept (bias) term to X\n",
    "X = np.hstack([X, np.ones((X.shape[0], 1))])  # shape (5, 3)\n",
    "\n",
    "# Initialize weights (2 features + 1 bias)\n",
    "W = np.zeros((X.shape[1], 1))  # shape (3, 1)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "n = X.shape[0]\n",
    "\n",
    "# Gradient Descent Loop\n",
    "for i in range(epochs):\n",
    "    y_pred = X @ W\n",
    "    error = y_pred - y\n",
    "\n",
    "    gradients = (2/n) * (X.T @ error)\n",
    "    W -= lr * gradients\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        loss = np.mean(error ** 2)\n",
    "        print(f\"Epoch {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "# Output\n",
    "print(\"\\nFinal Parameters (theta):\", W)\n",
    "print(\"Predictions:\", (X @ W))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
